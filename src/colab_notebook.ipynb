{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made easy to run in cloud like google, kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lightning wandb git+https://github.com/sampath017/extentions.git pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "from torchmetrics.functional import accuracy\n",
    "from lightning.pytorch import LightningModule\n",
    "from torch import nn\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from lightning.pytorch import (\n",
    "    callbacks,\n",
    "    loggers,\n",
    "    Trainer,\n",
    "    utilities\n",
    ")\n",
    "\n",
    "from model import Digits\n",
    "from data_module import MNISTDataModule\n",
    "from extentions.callbacks import DiffEarlyStopping, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = Path('../')\n",
    "\n",
    "dataset = MNIST(\n",
    "    root=(root_path / 'data').as_posix(),\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single data point\n",
    "for img, label in dataset:\n",
    "    print(img, img.getextrema())\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(label)\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A grid of images from each class\n",
    "\n",
    "dataset.transform = Compose([\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "num_classes = 10\n",
    "images_per_class = 10\n",
    "\n",
    "# Create a list to store the images for each class\n",
    "class_images = [[] for _ in range(num_classes)]\n",
    "\n",
    "# Iterate over the dataset and collect the first `images_per_class` images for each class\n",
    "for image, label in dataset:\n",
    "    if len(class_images[label]) < images_per_class:\n",
    "        class_images[label].append(image)\n",
    "    if all(len(images) == images_per_class for images in class_images):\n",
    "        break\n",
    "\n",
    "# Create a single grid of images with one row per class\n",
    "grid = make_grid([img for images in class_images for img in images], nrow=images_per_class)\n",
    "plt.imshow(grid.permute(1, 2, 0))\n",
    "\n",
    "# Add labels for each class at the start of the row\n",
    "for i in range(num_classes):\n",
    "    plt.text(-20, i * 31 + 17, str(i), fontsize=16)\n",
    "\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random images\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=7, shuffle=True)\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    grid = make_grid(images)\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "\n",
    "    # Add labels for the images\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.text(i * 32 + 8, -4, str(label.item()), fontsize=16)\n",
    "\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Class distributions\n",
    "\n",
    "# Get the labels for all images in the dataset\n",
    "labels = [label for _, label in dataset]\n",
    "\n",
    "# Count the number of occurrences of each label\n",
    "label_counts = Counter(labels)\n",
    "label_counts_array = np.array(list(label_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean: {label_counts_array.mean()}, STD: {label_counts_array.std()}\")\n",
    "\n",
    "# Plot the distribution of labels\n",
    "plt.bar(list(label_counts.keys()), list(label_counts.values()))\n",
    "plt.xticks(range(10))\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = nn.Sequential(\n",
    "    # Feature extractor\n",
    "    nn.Conv2d(1, 32, kernel_size=3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 128, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2),\n",
    "    nn.Dropout(p=0.25),\n",
    "\n",
    "    # learner\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(128, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "\n",
    "model_ffn = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(28*28, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 10)\n",
    ")\n",
    "\n",
    "\n",
    "class Digits(LightningModule):\n",
    "    def __init__(self, optimizer_name, optimizer_hparams={}):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model_cnn\n",
    "        # self.model = model_ffn\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.model(X)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y, task='multiclass', num_classes=10)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_acc\", acc*100.0, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y, task='multiclass', num_classes=10)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc*100.0, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self(X)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        acc = accuracy(y_pred, y, task='multiclass', num_classes=10)\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc*100.0)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        logits = self(batch)\n",
    "        probs = F.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams.optimizer_name == 'SGD':  # type: ignore\n",
    "            optimizer = torch.optim.SGD(\n",
    "                self.parameters(), **self.hparams.optimizer_hparams)  # type: ignore\n",
    "\n",
    "        elif self.hparams.optimizer_name == 'Adam':  # type: ignore\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(), **self.hparams.optimizer_hparams)  # type: ignore\n",
    "        else:\n",
    "            assert False, f'Unknown optimizer: \"{self.hparams.optimizer_name}\"'  # type: ignore # nopep8\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictMnist(Dataset):\n",
    "    def __init__(self, data_dir=Path(\".\"), transform=None):\n",
    "        # Store the image and its label as attributes\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.data = list(data_dir.iterdir())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.data[index])\n",
    "        img = img.convert(mode=\"L\")\n",
    "\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.batch_size = 64\n",
    "        self.cpu_count = c if (c := os.cpu_count()) else 8\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download (ontime process)\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            dataset = datasets.MNIST(\n",
    "                self.data_dir,\n",
    "                train=True,\n",
    "                transform=self.transforms\n",
    "            )\n",
    "            train_size = int(0.7 * len(dataset))\n",
    "            valid_size = len(dataset) - train_size\n",
    "\n",
    "            self.train_dataset, self.valid_dataset = random_split(\n",
    "                dataset,\n",
    "                [train_size, valid_size]\n",
    "            )\n",
    "\n",
    "        # Assign test dataset for use in dataloader's\n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = datasets.MNIST(\n",
    "                self.data_dir,\n",
    "                train=False,\n",
    "                transform=self.transforms\n",
    "            )\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.predict_dataset = PredictMnist(\n",
    "                self.data_dir/\"predict\", transform=self.transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.cpu_count)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset, batch_size=self.batch_size, num_workers=self.cpu_count)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.cpu_count)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size, num_workers=self.cpu_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.INFO)\n",
    "root_path = Path('../')\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"train.ipynb\"\n",
    "\n",
    "dm = MNISTDataModule(data_dir=(root_path / 'data').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Digits(\n",
    "    optimizer_name='Adam',\n",
    "    # optimizer_hparams={\n",
    "    #     'lr': 0.001,\n",
    "    #     'momentum': 0.9\n",
    "    # }\n",
    ")\n",
    "\n",
    "earlystopping_callbacks = [\n",
    "    DiffEarlyStopping(\n",
    "        monitor1=\"val_loss\",\n",
    "        monitor2=\"train_loss\",\n",
    "        diff_threshold=0.05, # like val_loss=0.09, train_loss=0.04\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_acc\",\n",
    "        min_delta=0.0,\n",
    "        mode='max',\n",
    "        stopping_threshold=99.99,\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filename=\"epoch={epoch}-loss={val_loss:.3f}\",\n",
    "    auto_insert_metric_name=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.model_summary.ModelSummary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = root_path/'logs'\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "logger = loggers.WandbLogger(\n",
    "    project='Digits',\n",
    "    save_dir=log_dir,\n",
    "    log_model='all',\n",
    ")\n",
    "\n",
    "max_time =  {'minutes': 20} if torch.cuda.is_available() else {'hours': 2}\n",
    "trainer = Trainer(\n",
    "    min_epochs=10,\n",
    "    max_epochs=50,\n",
    "    log_every_n_steps=1,\n",
    "    max_time=max_time,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback] + earlystopping_callbacks, # type: ignore\n",
    "    enable_model_summary=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=dm)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from lightning.pytorch import loggers, utilities, Trainer\n",
    "\n",
    "from model import Digits\n",
    "from data_module import MNISTDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Digits.load_from_checkpoint(model_path, map_location=torch.device('cpu'))\n",
    "utilities.model_summary.ModelSummary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    inference_mode=True,\n",
    "    logger=logger,\n",
    "    enable_model_summary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule=dm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
